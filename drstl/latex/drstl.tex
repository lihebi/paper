\documentclass[conference]{IEEEtran}
\usepackage{fancyhdr}
\usepackage{cite}
\usepackage{graphicx}
\ifCLASSINFOpdf
\else
\fi
\usepackage[font=footnotesize]{subfig}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{TreeSync: A Distributed Message Synchronization Algorithm Using Topology-Related Hierarchy over Named Data Network}
%\author{\IEEEauthorblockN{Hebi Li, Xiaobin Tan, Zijian Zhou, Zhifan Zhao}
%\IEEEauthorblockA{
%CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System\\
%University of Science and Technology of China\\
%Hefei, P.R. China\\
%Email: lihebi@mail.ustc.edu.cn, xbtan@ustc.edu.cn, zjzhou62@mail.ustc.edu.cn, zzfan@mail.ustc.edu.cn}
%}
\maketitle

% \lhead{Submitted to INFOCOM'2014 Workshop on Name-Oriented Mobility on 12/30/2013}
% \thispagestyle{fancy}

\begin{abstract}
Key problem of multi-user real-time communication applications,
such as group chat and video conference,
is how to synchronize messages among all participants.
Traditional IP-based way depends on central server, resulting in unbalanced link burden and robust problem.
In this paper, we propose a new distributed algorithm based on Named Data Network to address this problem.
In our design, the system generates a tree topology according to the real one.
Every node takes responsibility to synchronize its children and bubble messages up to its parent.
This method synthesizes the powerful control ability of servers and NDN's distributed features.
We implemented our design on NDNsim and it proved small overhead and fast synchronization.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

Applications dealing with multi-user communications,
like group chat and video conference,
needs efficient and robust way to synchronize among participants.
Traditionally in IP-based network,
all the participants should register on a central server,
send self-generated messages to it as well as fetching data back from it.
Due that IP-based network only provides support for point-to-point communication,
everyone should have to set up a connection with the server.
The server has to send a copy to every participant regardless of the fact that
they may be near each other in the topology,
resulting in performance problems, such as high burden of links and bandwidth,
extremely big overhead, etc.
What's worse, the whole system goes down when the server fails.

As one of the Future Internet Architecture(FIA)\cite{FIA},
Named Data Network(NDN)\cite{NDN001} goes over the basic downsides of IP-based network,
providing the basis of this new method to handle synchronization problems.
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item Instead of point-to-point connection, resources in NDN are identified by a unique name.
Consumers just send interests for the specific name rather than to a certain IP.
	\item In NDN, routers will filter the duplicated requests and only send one copy to the next hop.
	\item NDN routers will response to all incoming interests when receiving data from next hop.
	\item NDN router has the ability to cache data packet thanks to Content Store,
which enables the routers to response immediately for a same interest.
\end{enumerate}
Based on these features, it's time to get messages synchronized in distributed way
to gain small overhead and high robustness.

There is a distributed algorithm to deal with synchronization problem by researchers in UCLA, the
\emph{ChronoSync\cite{ChronoSync}}.
ChronoSync is a pure distributed protocol,
where participants in the system broadcast sync interests to the whole system.
Since the newly generated message can be responsed directly to the sync insterests,
it can be very fast for every one to receive it.
However, ChronoSync also suffers from several downsides.
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item There is no server, so no control ability when conflict happens.
For example, when there are simultaneous messages generated,
the existing sync interests may be consumed by different messages,
resulting in divide of the system.
	\item Sync Interest is broadcasted, so it is not scalable.
\end{enumerate}

Our Contribution includes:
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item proposed a new algorithm handling the problem of message synchronization using NDN.
	\item implemented our new protocol on ndnSIM\cite{ndnSIM}.
	\item evaluated the performance of our methods.
Demonstrated the merits of our design in comparison to ChronoSync.
\end{enumerate}

The rest of the paper is organized as follows.
We describe our protocol design in detail in section \ref{design}.
In section \ref{implementation},
we talk about the implementation on ndnSIM over NS3 platform and evaluate the performance.
Section \ref{related_work} shows related work,
and we conclude the paper in section \ref{conclusion}.

\section{Protocol Design}
\label{design}

\subsection{NDN Background}
This section briefly goes through the necessary Named Data Network architecture background in support for our protocol.
In NDN, communication is consumer driven.
Consumer sends out interest to fetch the data back.
Every piece of data is specified by its uique name. When a router receives an interest,
it will first check whether the desired data is in its \emph{Content Store(CS)}.
If the data packet is already cached in Content Store,
the router will drop the interest and return the data packet.
If not, the router checks the \emph{Pending Interest Table(PIT)}.
In the case the record for this interest is in a router's PIT,
the router simply drops the interest and adds an interface to this PIT record.
Otherwise the router will finally check the \emph{Forwarding Interest Table(FIB)}
by means of \emph{Longest Prefix Match} to determine which interface to forward the interest to.
When it still can't find the right way to forward the interest, it drops the interest.

When the publisher receives the interest for his data,
he will send the data packet with the same name. On receiving a data packet,
the router will check its PIT to see which interface comes the interest for this data.
When found, the router forwards the data packet to all the interfaces in the PIT
corresponding to the data name and then deletes this PIT record.
After that, the router will cache the data packet in its Content Store
so that it can satisfy the interest for the same data in the future.
In the case that there is no record in the PIT for this data,
it means that no one sends the interest for this data or the interest timed out,
so it can cache the data packet in its Content Store or just simply drop it.
In this way, data packets will be forwarded back to all the consumers in the reverse path of interest packets.

\subsection{Design Goals}
We kept several goals when designing the protocol.
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item No central servers. We shoud make sure that it is a distributed robust protocol.
	\item Has ability to handle simultaneous data generation.
	\item Scalable.
\end{enumerate}

To achieve these, we design a tree topology. Each node is the controller of its children nodes.
Multi tiers make sure that it is scalable.
Each controller is selected by the system according to topology,
and the root of the tree is just someone who is selected multi times, nothing else special.
If a controller fails, one of his children takes his place.
Finally, since every controller is in charge of the control message synchronization,
the simultaneous data generation will not become a problem.

\subsection{Overview}

In this section, we are going to talk about the whole synchronization process briefly.
We will discuss every part in detail in the following sections.

\subsubsection{Tree Topology}
When someone join a system, the first thing for him is to find his position in the topology.
At the very beginning,
veryone waits for a specific random time before sending \emph{Any Server Interest} inquiry.
If he doesn't receive a reply, he will assume himself the controller,
and thus ready to satisfy the same interest from others.
On receiving this reply, a node will set its parent and request control messages from it.
The \emph{Any Server Interest} is labeled by a specific level to enable multi-level controller.

Everyone sends a heart beat interest to his server to check whether the server is still available.
Once the server failure is detected,
a new server that substitutes him will be selected from the clients in the same group.
A new participant can find a controller to be its parent in his neighborhood,

\subsubsection{Control Message}
Controllers control the sync process by what we call \emph{Control Message}(CM for short).
CMs are not real data, but the packet to tell when and where the data is.
Besides the actual data's NDN name, the CM keeps a label.
When CM propagates down the tree,
each controller insert his name and the time he receive it to the label.
As a result, a node will receive a CM containing the name and time of all its parents.

\subsubsection{Synchronization}
When a participant knows something new,
he will propagate it down to his clients and bubble it to his parents.

\subsubsection{Bubble Up}
In the case a node has something new to tell,
it sends interest together with the true data name to inform his parent about it.
When the controller receives such interest,
he adds his name and time to the label of this record and save it in local log.
If he also has a parent, he bubbles this up in the same way.

\subsubsection{Propagation Down}
In order to get latest control messages,
every participant sends sync interest, which we call \emph{Anything New Interest},
to its server, with the latest labels he knows.
When the sync interest times out, a same interest is resent.
On receiving the sync interest, the server compares the label with his own.
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item If the label's time is the same as his,
it is saved as pending interest for the sake of immediate reply when status updates.
	\item If the label is older than his latest label,
he will find all the new records after the specific time and send them back in data packet.
\end{enumerate}
On receiving response of sync interest,
a client propagates it down by satisfying the sync interest from his clients,
thus forwards the CM to the whole system.

\subsubsection{Actual Data Fetching}
On receiving CM, everyone sends out interest for the specific data name directly.
Since that the name of every piece of data is stable in NDN,
the interest from every participant for the same data
will be aggregated by routers ensuring no duplicated interests in one link.
When the data packet is returned from the publisher,
it travels the reverse path of the interest to all the receiver,
resulting in small overhead. Besides, the data packet can be cached in router's Content Store,
so when there is another participant requesting for the same data,
he will get it in the router instead of the publisher.

\subsection{Naming Rules}

Naming is one of the most important parts of NDN applications.
In this section, we briefly go through all the interest names that we use in this design,
and they will be discussed thoroughly in the following sections for details.

There are four interest names in our design, as in Table \ref{interest_name}.
\emph{Any Server Interest}(ASI) is used for server generation.
\emph{Something New Interest}(SNI) and \emph{Anything New Interest}(ANI) are responsible for synchronizing control messages.
\emph{Data Interest}(DI) serves for actual data fetching.
The first part is used for router to forward the interest packet correctly.
\emph{`/broadcast'} interest is routed to all the corresponding interfaces.
\emph{`/serverName'} and \emph{`/publisher'} are routed to specific nodes.
The second component, \emph{`/treesync'} is a specific label for the application.
\footnote{To inserting FIBs into routers, the client publishes the interest name,
declaring that they can reply to such interest.
The router protocols, such as OSPF\cite{OSPF}\cite{OSPFN},
are responsible to inform the other routers this information.
ndnSIM provides a function to set the global scale routing FIBs for every node,
so in our simulation, we use this to simulate this action}

Other components of the names are specified by the procedures,
and we will cover them in the following sections in detail.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Interests' Names}
\label{interest_name}
\centering
\begin{tabular}{|c|c|}
\hline
Any Server Interest & /broadcast/treesync/anyserver/\emph{level}\\
\hline
Something New Interest & /\emph{serverName}/treesync/somethingnew/\emph{mylabel}/\emph{dataName}\\
\hline
Anything New Interest & /\emph{serverName}/treesync/anythingnew/\emph{label1:label2:label3}\\
\hline
Data Interest & /\emph{publisherName}/treesync/\emph{time}\\
\hline
\end{tabular}
\end{table}


\subsection{Tree Topology Generation}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/server-generation.png}
\caption{Server Generation}
\label{server_generation}
\end{figure}
To generate servers, the \emph{Any Server Interest} is used.
The first two components of the name is used for router forwarding and interest handling.
The third one is the interest type.
The last element \emph{`level'} is a number indicating in which level the sender currently is.

In the very beginning,
everyone doesn't have a server.
They will individually wait for a random time and broadcast
ASI to inquire whether there is a server nearby.
Then they wait a time $T_s$ for the reply.
On receiving the data containing the server name,
the participant will set his server and request for CM from it later.
Otherwise, if he doesn't receive a data after Ts,
he assumes himself as the server and thus is ready to reply the such interests from others.
When the region is large, there should be many groups, each has a selected server.
These servers will obey the same rules, wait for a random time,
and broadcast an ASI with their current level.
After sending this interest,
the sender waits a time related to his current level
\footnote{This time is a function of current level, because the higher the level is,
the larger its region should be, the longer wait time for Any Server Inquiry will be.
So the wait time can control the region's area for a particular level.
In implementation, interest sender ignores data packet received after this time.
However, it is better to support to set the interest life time(hop count) in the low level
so that there will not be extra interest sending to unreachable areas.} for the data.
If he doesn't receive it after the time expire, he increase his level.
The one who receives this interest will compare this level with his own.
If this one is lower, he will send data packet to satisfy this interest.
We can set a small constant number as the Top Level.
Whenever a participant has not a server and he is not a top level server,
he will act as above in server generation period.
If a participant already has a server,
he sends heart beat interest to his server to detect possible failure.
We will cover failure reversion in Section \ref{trouble_shooting}.

Noting that the server generation procedure is closely related to the reply delay,
so the randomly selected multi-level servers are closely related to the topology as well.
The direction of transmission is related to the topology,
thus it is smart enough not to forward the messages in round.
This provides high performance for transferring CM between clients and servers.

\subsection{Synchronization}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/synchronization.png}
\caption{Synchronization and Data Fetching}
\label{synchronization}
\end{figure}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/tree-synchronization.png}
\caption{Multi-Level Synchronization}
\label{tree_synchronization}
\end{figure}
We describe how to synchronize in the system in this section.
As shown in Figure \ref{synchronization}, a client sends SNI to inform server a new change.
erver then reply pending ANI with the new change.
Participants fetch actual data in distributed way.
Figure \ref{tree_synchronization} shows multi-level synchronization.
A node will transfer the new status to upper and down level,
through sending SNI and replying ANI respectively.

\subsubsection{Something New Interest}
Once a node generates a new message, or he receives a SNI from his client,
he will bubble this up by sending this interest to his server with the actual data name.

When a participant generates a new data, he will do the following things:
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item adds the data to his Message Container.
	\item adds his own label to the record and stores it in his local Record Container.
	\item sends SNI with his label and the actual data name to his server
	\item processes pending interest.
\end{enumerate}
On receiving a SNI, a server  will do the following things:
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item removes the labels in this record
\footnote{because every node only keeps the labels from his upper level servers.},
, add his own label and store it into local Record Container.
	\item sends Something New Interest with his label and the actual data name to his server, if has.
	\item processes pending interest
	\item sends interest with the data name to fetch actual data.
\end{enumerate}
Note that SNI contains the actual data name in the end,
so once receiving this interest,
a node can immediately send this status change to upper and lower level without extra delay.

\subsubsection{Anything New Interest}
To get synchronization control records,
every node sends periodically ANI to his server with the newest label on behalf of his status.
When a server receives this interest,
he compares the carried label with his own latest label. There are three cases here:
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item The received label is the same as his own.
In such case, he saves this interest as pending interest,
so that he can reply immediately when his status changes.
	\item The received label is older than the his own.
In this manner, he compares the record's time and find the first index
in the Record Container that is newer than the given label.
Then he sends back the new records.
	\item There is not any record in his Record Container
that shares the same user name as the received label.
In this situation, the server has to send back all the records for synchronization.
\end{enumerate}
When the client receives data for Anything New Interest, he will do the following things:
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item extracts records from the data.
	\item updates his own Record Container.
	\item processes pending interest.
	\item sends data interest to fetch real data.
\end{enumerate}

In general, a server is in charge of inner group synchronization,
so every record in the group contains a label of the server.
So in regular case, the clients only send his server's latest label.
When the system is stable and all nodes share the same status,
all ANIs are the same and thus they can be aggregated by NDN routers.
Data back can transmit efficiently along the reverse path of interest.
When a node moves to another place or the server fails, the node will connect to a new server.
In this situation, the last server's label is of no use,
so he sends all the latest labels to the new server representing his status.
This model can provide natural support for robustness and mobility,
which will be covered in section \ref{mobility}.
seless out of date labels will be deleted by some algorithm such as LRU.

\subsection{Distributed Actual Data Fetching}
The servers are only in charge of synchronizing control records,
in other words the servers tell the participants in the group when and where to fetch a data.
Actual data fetching is acted in distributed way.
This design minimizes the messages that travel between server and client,
and takes fully advantages of NDN's distributed features such as interest aggregation and caches.

When receiving SNI or data from ANI,
a participant sends DI to fetch actual data.
As described above, everyone stores his self-generated data for the sake of replying others' data fetching interest.
They don't need to store the fetched data from others,
because they only care about whether they have received it or not.

\subsection{Robustness and Mobility Support}
\label{mobility}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/mobility.png}
\caption{Robustness and Mobility Support}
\label{mobility_pic}
\end{figure}
Our proposed model can provide natural robustness and mobility support.

When a node goes off-line and returns after some time and connects to the same server,
he will send his latest status, which can be recognized easily by the server,
since that the server can just compare the time in the label with his own record numerically,
find all new messages, and reply to the client.
When a node move, whether it is caused by link failure,
server failure or the actual move in topology, there are three situations.
\begin{enumerate}{\IEEEsetlabelwidth{12}}
	\item he moves to another place, but connects to the same server.
In this manner, he can communicate with the server and fetch new things without any affection.
	\item he moves to another place, but connects to a different server.
Chances are that he and the new server share a same upper level server
\footnote{actually the Top Level Server is always shared by the nodes,
but chances are high that a shared upper level server not far away is available
because server selection is closely related to topology.}.
Although he could not use his former server's labels to communicate with the new server,
he can use the shared upper level server's labels with the same performance.
For instance, as shown in Figure \ref{mobility_pic},
assume that Mary moves close to Bob and connects to him.
She can not use her former server, Cindy's label now.
However, she and Bob hold Alice's label together, thus Bob can still recognize her.
	\item When the node is moved to a completely new environment that even the Top Level Server is not the same.
This may be caused by contemporary partition or permanent one,
which means the two groups never connect to each other. This situation is extreme,
and the node has to exchange all the messages.
However, in deed it is not reasonable to handle this condition.
\end{enumerate}

\subsection{Trouble Shooting}
\label{trouble_shooting}
Clients send heat beat interest to detect connection failure.
When detected, the client will reset his server and enter server generation period.
There are two situations here, the server is down and the client is down.

In the first manner, the server is off-line,
so the clients in the group will soon select another one to substitute him after the detection.
If there are higher level servers,
the newly selected server will surely connect to the upper server by sending another
ASI with a bigger level, reverting the system to stable architecture.

In the second one,
we consider a more general condition that a small group disconnected
with the outside world for a while.
The participants inside the group can still communicate with each other in this case,
and the highest level server in this group will grow to be a \emph{Top Level Server}.
When the connection heals,
the group should be able to connect to the outside world again and
exchange messages instead of remaining a close system with its own \emph{Top Level Server}.
To solve this problem, a server can broadcast an interest inquiring if there is a same level server nearby.
If so, the server will decrease his level,
set his server to the newly detected server and tell his client to connect to the new server too.

\section{Implementation and Evaluation}
\label{implementation}
%Overview goes here
We implemented our protocol on ndnSIM,
an overlay of Network Simulation 3(NS3)\cite{ns3}.
As a comparison, we implemented another new protocol handling status synchronization problem,
the ChronoSync. This section, we will first confirm the rightness of the logic.
Then we will evaluate the performance on both packet overhead and delay.

\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/paper-topo.png}
\caption{5x5 Grid Topology and Generated Servers}
\label{paper_topo}
\end{figure}
To be more general, We use the 5x5 grid topology in our simulation,
as shown in Figure \ref{paper_topo}.
Every node in the grid represents a participant in the synchronization system,
thus there are totally 25 nodes.
The figure may seem simple,
but it actually contains nodes with 6 different environments and a max hop count of 8.
We think this topology can represent a general topology well.
The tree topology is randomly selected, as described before.
All links are symmetrical with link delay of 10ms and 1Mbps bandwidth.
We let participants randomly generate data,
and we change the frequence of the generation to see the trend.
Everyone's message generation period is independent.

\subsection{Functional Correctness}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/function-delay.png}
\caption{Delay for every node to receive data}
\label{function_delay}
\end{figure}
\subsubsection{Server Generation}
In the grid topology, the server generation works well.
There forms a four-level architecture, with the top level server on node(3,2).
The formed architecture should be better if the topology is random, not symmetrical.

\subsubsection{Message Synchronization}
All messages are successfully transfered to every participants.
Figure \ref{function_delay} shows the delay of the packages at the frequence of $\frac{2}{7}$Hz.
From the graph, we can see that delay is divided into three parts,
one very quickly, one average, and the other is a little slow.
That is related to the level architecture.
If the receiver is close to the publisher in level topology,
he should get the message quickly and sends out real data interest.
However, in the case that these two participants are far away in the hierarchy tree
(chances are high that they are far from each other in the topology too),
the synchronization control messages should go through several servers to reach to the other, resulting the delay.

\subsection{Evaluation}

\subsubsection{Overhead}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/all-incoming-interest-revised.png}
\caption{Incoming Interest Count for All the Nodes}
\label{overhead}
\end{figure}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/portion-in-interests.png}
\caption{Portion in Interests}
\label{recovery_percentage}
\end{figure}
Our model holds advantage for control,
thus the overhead is small. In this experiment,
we change the data generation speed and see the overhead change.
As shown in Figure \ref{overhead},
as the frequency enhances, our design holds a linear increase in packet delivery count,
while ChronoSync suffers a big increase as the frequency goes up,
because when the participants generate message more quickly,
chance of simultaneous data generation will increase,
resulting in ChronoSync applications send out recovery interests to solve the problem.
Note that we use all \emph{Incoming Interest}s,
because it can represent to the status.
The aggregation of NDN router will decline \emph{Incoming Interest} count too.
From the view of proportion that consists the interests,
as you can see in Figure \ref{recovery_percentage},
the percentage that the receiver will eventually send recovery interest
when receiving a sync interest is much higher when data generation frequency increase.
However, Something New Interest remains the same ratio with Anything New in TreeSync.

\subsubsection{Delay}
Since control messages should go through multi-level servers in our design,
in spite that the level is related to topology,
the performance is still lower than ChronoSync,
in which data always returns along the best way.
However, as a sacrifice, when the data is generated frequently,
ChronoSync applications should have to send out recovery interests,
resulting in a bigger delay. Figure \ref{delay_compare}
shows the delay as a function of frequency.
In the graph, we can see that the delay is a little bigger than ChronoSync,
but is still pretty good. Take the gained high control ability into consideration,
the sacrifice is worthy.
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/delay-compare-revised.png}
\caption{Delay Compare}
\label{delay_compare}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/data-fetch-delay.png}
\caption{Data Fetching Delay}
\label{data_fetch_delay}
\end{figure}
Another point, the data fetching procedure is distributed,
thus can take fully advantages of NDN architecture.
Figure \ref{data_fetch_delay} shows the delay from a participant
receives synchronization control message to when he gets the desired data.
It is clear that this procedure is very fast,
and it contributes to small delay and overhead.

\section{Related Work}
\label{related_work}
Zhenkai Zhu et al recently introduced a new way to handle this problem on NDN,
which is an entirely distributed and serverless protocol, named ChronoSync.
In ChronoSync, status is represented using the digest of all participants' current status.
Every participant keeps a digest tree and generates the root digest.
If all the root digests are the same in the system, then obviously the system status is stable.
To recovery from exceptions, every node keeps a digest log that contains the old digest and the trigger events.
Everyone broadcasts a sync interest periodically to the system with its current root digest.
Whoever receives a sync interest will compare the digest with his own root digest.
If it is the same, he keeps it as a pending digest.
If it is in his digest log, he sends back data containing the trigger events.
On receiving the data, the receiver knows what data is missing, thus send interest for that piece of data.

Since ChronoSync is entirely distributed,
and messages go along the optimized way, it almost has the smallest delay.
The main downside of ChronoSync is short of control ability,
resulting in problems to handle frequent simultaneous data generation.
When this happens, the system will be divided into two different group and can't recognize digest from each other.
The solution of ChronoSync is to send recovery interest with the unknown digest.
The one who can recognize this digest will send all his current statuses of the whole system back
so that the receiver could compare the status with his own to find out what's missing.
It should work fine, but in the case that the system is large,
or data generation is frequent, video conference for example,
there will be lots of simultaneous data generation.

\section{Conclusion}
\label{conclusion}
In this paper, we proposed a new protocol to handle the synchronization problem in multi-user applications.
TreeSync takes advantages of both traditional server-based model and NDN's neat and efficient distributed features.
First of all, Multi-level controllers can provide enough control ability to handle complex conditions;
Secondly, The essential distributed features allow it to fetch data efficiently and robustly with little overhead;
Besides, The hierarchy structure makes the algorithm scalable.
We have implemented TreeSync over ndnSIM and evaluated the performance.
Message is correctly and fast synchronized in a distributed way.
With a reasonable delay of sync control message transmission,
the overhead is much lower due to powerful control ability.

Some further topics worth researching include:
(1) minimizing within-level delays,
(2) generating sever by some better algorithms to generate a hierarchy structure
that best fits each specific topology.

\end{document}

\begin{thebibliography}{1}

\bibitem{FIA}
``NSF Future Internet Architecture Project'', http://www.nets-fia.net
\bibitem{NDN001}
Zhang, L., Estrin, D., Burke, J., Jacobson, V., Thornton, J. D., Smetters, D. K., ... \& Yeh, E. (2010).
Named data networking (ndn) project. Relatório Técnico NDN-0001, Xerox Palo Alto Research Center-PARC.
\bibitem{ChronoSync}
Zhu, Z., \& Afanasyev, A. (2013).
Let’s ChronoSync: Decentralized Dataset State Synchronization in Named Data Networking.
In Proceedings of the 21st IEEE International Conference on Network Protocols (ICNP 2013).
\bibitem{ndnSIM}
Afanasyev, A., Moiseenko, I., \& Zhang, L. (2012).
ndnSIM: NDN simulator for NS-3. University of California, Los Angeles, Tech. Rep.
\bibitem{ns3}
``ns-3: a discrete-event network simulator for Internet systems.'', http://www.nsnam.org
\bibitem{OSPF}
Moy, J. (1998). Open shortest path first (ospf) version 2. IETF: The Internet Engineering Taskforce RFC, 2328.
\bibitem{OSPFN}
Wang, L., Hoque, A. K. M. M., Yi, C., Alyyan, A., \& Zhang, B. (2012).
OSPFN: An OSPF based routing protocol for Named Data Networking.
University of Memphis and University of Arizona, Tech. Rep.
\bibitem{LRU}
Johnson, T., \& Shasha, D. (1994).
X3: A Low Overhead High Performance Buffer Management Replacement Algorithm.
\end{thebibliography}

\end{document}


% obsolate
\section{obsolate}
\label{sec:section label}

\subsection{Data Structure}
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{../png/data-structure.png}
\caption{Data Structure}
\label{data_structure}
\end{figure}
In this section,
we describe the data structure used for storing and processing synchronization messages.

As shown in Figure \ref{data_structure}, there are two data structures in every node, name
y Message Container and Record Container.
Message Container stores the actual data generated by this participant.
Keep in mind that participant doesn't need to store everyone else's messages.
They only care about whether they have received all the others' messages.
However, it is the publisher's responsibility to store his own published messages
for the sake of replying the actual data request interest from others.
Record Container holds the synchronization control records.
A timed label is the pair of a user name and his time.
A record holds a data name and all the labels on behalf of the dataname.
A record container consists of two parts, namely \emph{Record Vector} and \emph{NameTimeIndex}.
The \emph{Record Vector} is an array of the records by sequence,
so that their indexes can tell which is newer.
The \emph{NameTimeIndex} structure is a dual map.
It is use to find the first index that is newer than a provided label contains a name and time.
The index is the one that used in \emph{Record Vector}.

Note that every label about a record contains a user name and user time,
thus there's no problem when different participants use different time criteria.
Also, since that the record only contains labels from the participants' upper-level servers,
and the top level is a small constant number, we don't need to worry about a huge amount of labels.
What's more, when a server is down, his labels will never be updated again,
so we can use some algorithm like Least Recent Use(LRU)\cite{LRU} to delete useless labels. One more thing,
this design provide support for node failure and node mobility, which is referred in Section \ref{mobility}.
